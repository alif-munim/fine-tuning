{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/alif-munim/language-models/blob/main/lora/lora_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGBSz9As3qS4"
   },
   "source": [
    "# LoRA\n",
    "Using LoRA to fine-tune on a single digit after training an MNIST classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KkLV-rU63pYY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W7oOQ03zjzjp"
   },
   "outputs": [],
   "source": [
    "# Make model deterministic\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9nibyyrH35V0"
   },
   "outputs": [],
   "source": [
    "# Load MNIST dataset and create dataloaders\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "XgjbMbIW4h5Y"
   },
   "outputs": [],
   "source": [
    "# Create an over-parameterized, inefficient neural network for classification\n",
    "class BigNet(nn.Module):\n",
    "  def __init__(self, hidden_dim1=1000, hidden_dim2=2000):\n",
    "    super(BigNet, self).__init__()\n",
    "    self.linear1 = nn.Linear(28*28, hidden_dim1)\n",
    "    self.linear2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "    self.linear3 = nn.Linear(hidden_dim2, 10)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "  def forward(self, img):\n",
    "    x = img.view(-1, 28*28)\n",
    "    x = self.relu(self.linear1(x))\n",
    "    x = self.relu(self.linear2(x))\n",
    "    x = self.linear3(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Z9jtovEw5q9-"
   },
   "outputs": [],
   "source": [
    "# Define training loop for model on MNIST\n",
    "\n",
    "def train(model, train_loader, num_epochs, iter_limit=None):\n",
    "  cross_el = nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "  total_iters = 0\n",
    "  for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    num_iters = 0\n",
    "\n",
    "    data_iterator = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n",
    "    if iter_limit is not None:\n",
    "      data_iterator.total = iter_limit\n",
    "\n",
    "    for data in data_iterator:\n",
    "      num_iters += 1\n",
    "      total_iters += 1\n",
    "\n",
    "      x, y = data\n",
    "      x = x.to(device)\n",
    "      y = y.to(device)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      output = model(x.view(-1, 28*28))\n",
    "      loss = cross_el(output, y)\n",
    "      loss_sum += loss.item()\n",
    "      avg_loss = loss_sum / num_iters\n",
    "      data_iterator.set_postfix(loss=avg_loss)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      if iter_limit is not None and total_iters >= iter_limit:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_6wyNGOC7GYn",
    "outputId": "412b0120-e256-4295-871e-5da9bdbba05f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 6000/6000 [00:45<00:00, 131.84it/s, loss=0.238]\n"
     ]
    }
   ],
   "source": [
    "# Train model for one epoch to simulate large-scale pre-training\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BigNet().to(device)\n",
    "\n",
    "train(model, train_loader, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "0F503Li27M3F"
   },
   "outputs": [],
   "source": [
    "# Clone original weights\n",
    "\n",
    "original_weights = {}\n",
    "for name, param in model.named_parameters():\n",
    "  original_weights[name] = param.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "takNNVcP8TU4",
    "outputId": "7b21416c-bcc9-402d-dc7b-ab1a3018512d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:03<00:00, 317.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.956\n",
      "Wrong counts for digit 0: 9\n",
      "Wrong counts for digit 1: 7\n",
      "Wrong counts for digit 2: 32\n",
      "Wrong counts for digit 3: 55\n",
      "Wrong counts for digit 4: 74\n",
      "Wrong counts for digit 5: 38\n",
      "Wrong counts for digit 6: 53\n",
      "Wrong counts for digit 7: 63\n",
      "Wrong counts for digit 8: 62\n",
      "Wrong counts for digit 9: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the performance of the pretrained model on the test dataset\n",
    "\n",
    "def test(model, test_loader):\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  wrong_counts = [0 for i in range(10)]\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for data in tqdm(test_loader, desc='Testing'):\n",
    "      x, y = data\n",
    "      x = x.to(device)\n",
    "      y = y.to(device)\n",
    "      output = model(x.view(-1, 28*28))\n",
    "\n",
    "      for idx, i in enumerate(output):\n",
    "        if torch.argmax(i) == y[idx]:\n",
    "          correct += 1\n",
    "        else:\n",
    "          wrong_counts[y[idx]] += 1\n",
    "        total += 1\n",
    "\n",
    "    print(f'\\nAccuracy: {round(correct/total, 3)}')\n",
    "    for i in range(len(wrong_counts)):\n",
    "      print(f'Wrong counts for digit {i}: {wrong_counts[i]}')\n",
    "\n",
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LbOpZWdZ9lo6",
    "outputId": "c2efa2e4-c5ca-489c-c85e-62bed5a177dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000])\n",
      "Layer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000])\n",
      "Layer 3: W: torch.Size([10, 2000]) + B: torch.Size([10])\n",
      "Total number of parameters: 2,807,010\n"
     ]
    }
   ],
   "source": [
    "# Print the size of the model's weight matrices and total parameters\n",
    "original_params = 0\n",
    "for index, layer in enumerate([model.linear1, model.linear2, model.linear3]):\n",
    "  original_params += layer.weight.nelement() + layer.bias.nelement()\n",
    "  print(f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape}')\n",
    "print(f'Total number of parameters: {original_params:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5sT6J6qVFZeB",
    "outputId": "45addd1d-123d-48e7-b1b1-f9f080557fe8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "qA9ct3BEA21P"
   },
   "outputs": [],
   "source": [
    "# Define LoRA parameterization as defined in the paper\n",
    "\n",
    "class LoraParametrization(nn.Module):\n",
    "  def __init__(self, features_in, features_out, rank=1, alpha=1, device='cpu'):\n",
    "    super().__init__()\n",
    "\n",
    "    self.lora_B = nn.Parameter(torch.zeros((features_in, rank)).to(device))\n",
    "    self.lora_A = nn.Parameter(torch.zeros((rank, features_out)).to(device))\n",
    "\n",
    "    # Use a random gaussian for A and zero for B so ΔW = BA is zero initially\n",
    "    nn.init.normal_(self.lora_A, mean=0, std=1)\n",
    "\n",
    "    # Introduce a scaling term. Set alpha to the first r we try (in this case 1)\n",
    "    # No need to tune it afterwards, even with different values for r\n",
    "    self.scale = alpha / rank\n",
    "    self.enabled = True\n",
    "\n",
    "  def forward(self, original_weights):\n",
    "    if self.enabled:\n",
    "      return original_weights + torch.matmul(self.lora_B, self.lora_A).view(original_weights.shape) * self.scale\n",
    "    else:\n",
    "      return original_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "iLJze7veC6Gr"
   },
   "outputs": [],
   "source": [
    "# Add parametrization to the linear layers\n",
    "\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "\n",
    "def linear_layer_parametrization(layer, device, rank=1, lora_alpha=1):\n",
    "  # Only add to weight matrix, not bias\n",
    "  features_in, features_out = layer.weight.shape\n",
    "  return LoraParametrization(\n",
    "      features_in, features_out, rank=rank, alpha=lora_alpha, device=device\n",
    "  )\n",
    "\n",
    "parametrize.register_parametrization(\n",
    "    model.linear1, \"weight\", linear_layer_parametrization(model.linear1, device)\n",
    ")\n",
    "\n",
    "parametrize.register_parametrization(\n",
    "    model.linear2, \"weight\", linear_layer_parametrization(model.linear2, device)\n",
    ")\n",
    "\n",
    "parametrize.register_parametrization(\n",
    "    model.linear3, \"weight\", linear_layer_parametrization(model.linear3, device)\n",
    ")\n",
    "\n",
    "def enable_disable_lora(enabled=True):\n",
    "  for layer in [model.linear1, model.linear2, model.linear3]:\n",
    "    layer.parametrizations[\"weight\"][0].enabled = enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yeHhGo7qEzJr",
    "outputId": "a12d7ecd-e0d5-4acd-8eb3-a365ff2fd171"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParametrizedLinear(\n",
       "  in_features=784, out_features=1000, bias=True\n",
       "  (parametrizations): ModuleDict(\n",
       "    (weight): ParametrizationList(\n",
       "      (0): LoraParametrization()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zpqJ1xEpE0Zc",
    "outputId": "188e798f-3820-46f2-f05c-8b0bd88710f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: \n",
      "\tW: torch.Size([1000, 784]) \n",
      "\t+ B: torch.Size([1000]) \n",
      "\t+ LoRA_A: torch.Size([1, 784]) \n",
      "\t+ LoRA_B: torch.Size([1000, 1])\n",
      "Layer 2: \n",
      "\tW: torch.Size([2000, 1000]) \n",
      "\t+ B: torch.Size([2000]) \n",
      "\t+ LoRA_A: torch.Size([1, 1000]) \n",
      "\t+ LoRA_B: torch.Size([2000, 1])\n",
      "Layer 3: \n",
      "\tW: torch.Size([10, 2000]) \n",
      "\t+ B: torch.Size([10]) \n",
      "\t+ LoRA_A: torch.Size([1, 2000]) \n",
      "\t+ LoRA_B: torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "# Compare the total number of parameters added to the model by LoRA\n",
    "lora_params = 0\n",
    "non_lora_params = 0\n",
    "\n",
    "for index, layer in enumerate([model.linear1, model.linear2, model.linear3]):\n",
    "  lora_params += layer.parametrizations[\"weight\"][0].lora_A.nelement() + layer.parametrizations[\"weight\"][0].lora_B.nelement()\n",
    "  non_lora_params += layer.weight.nelement() + layer.bias.nelement()\n",
    "  print(\n",
    "      f'Layer {index + 1}: \\n\\tW: {layer.weight.shape} \\n\\t+ B: {layer.bias.shape} \\n\\t+ LoRA_A: {layer.parametrizations[\"weight\"][0].lora_A.shape} \\n\\t+ LoRA_B: {layer.parametrizations[\"weight\"][0].lora_B.shape}'\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GlOyxBDDGcSW",
    "outputId": "db9c9121-f7e5-4c0c-8952-922e08493473"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters (original): 2,807,010\n",
      "Total number of parameters (original + LoRA): 2,813,804\n",
      "Parameters introduced by lora: 6,794\n",
      "Increase in parameters: 0.242%\n"
     ]
    }
   ],
   "source": [
    "assert non_lora_params == original_params\n",
    "print(f'Total number of parameters (original): {non_lora_params:,}')\n",
    "print(f'Total number of parameters (original + LoRA): {lora_params + non_lora_params:,}')\n",
    "print(f'Parameters introduced by lora: {lora_params:,}')\n",
    "\n",
    "param_increase = (lora_params / non_lora_params) * 100\n",
    "print(f'Increase in parameters: {param_increase:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KQNvUSipHqxO",
    "outputId": "89714556-c590-4658-c379-1fdc09918f21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.bias\n",
      "linear1.parametrizations.weight.original\n",
      "linear1.parametrizations.weight.0.lora_B\n",
      "linear1.parametrizations.weight.0.lora_A\n",
      "linear2.bias\n",
      "linear2.parametrizations.weight.original\n",
      "linear2.parametrizations.weight.0.lora_B\n",
      "linear2.parametrizations.weight.0.lora_A\n",
      "linear3.bias\n",
      "linear3.parametrizations.weight.original\n",
      "linear3.parametrizations.weight.0.lora_B\n",
      "linear3.parametrizations.weight.0.lora_A\n"
     ]
    }
   ],
   "source": [
    "# View named parameters in model\n",
    "for name, param in model.named_parameters():\n",
    "  print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cvU3UCJNG9fK",
    "outputId": "7c173ed4-4a3b-4b19-fe52-bec4d9891b75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing non-LoRA parameter linear1.bias...\n",
      "Freezing non-LoRA parameter linear1.parametrizations.weight.original...\n",
      "Freezing non-LoRA parameter linear2.bias...\n",
      "Freezing non-LoRA parameter linear2.parametrizations.weight.original...\n",
      "Freezing non-LoRA parameter linear3.bias...\n",
      "Freezing non-LoRA parameter linear3.parametrizations.weight.original...\n"
     ]
    }
   ],
   "source": [
    "# Freeze non-LoRA network params\n",
    "\n",
    "frozen_params = []\n",
    "for name, param in model.named_parameters():\n",
    "  if 'lora' not in name:\n",
    "    print(f'Freezing non-LoRA parameter {name}...')\n",
    "    param.requires_grad = False\n",
    "    frozen_params.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yDXc5T_VIY4p",
    "outputId": "dd185d85-9bf8-4dcc-875b-c1c92c846534"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  99%|█████████▉| 99/100 [00:00<00:00, 128.43it/s, loss=0.0594]\n"
     ]
    }
   ],
   "source": [
    "# Only fine-tune LoRA params for digit 4 for 100 batches\n",
    "\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "exclude_indices = mnist_trainset.targets == 4\n",
    "mnist_trainset.data = mnist_trainset.data[exclude_indices]\n",
    "mnist_trainset.targets = mnist_trainset.targets[exclude_indices]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "train(model, train_loader, num_epochs=1, iter_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CcIsLABHuZk"
   },
   "outputs": [],
   "source": [
    "# Check frozen params are the same after fine-tuning\n",
    "\n",
    "assert torch.all(model.linear1.parametrizations.weight.original == original_weights['linear1.weight'])\n",
    "assert torch.all(model.linear2.parametrizations.weight.original == original_weights['linear2.weight'])\n",
    "assert torch.all(model.linear3.parametrizations.weight.original == original_weights['linear3.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "FHP2jisYKaEG"
   },
   "outputs": [],
   "source": [
    "# Check that pytorch replaces weight access by LoRA parametrization\n",
    "\n",
    "enable_disable_lora(enabled=True)\n",
    "old_weights = model.linear1.weight\n",
    "\n",
    "scale_term = model.linear1.parametrizations.weight[0].scale\n",
    "lora_term = (model.linear1.parametrizations.weight[0].lora_B @ model.linear1.parametrizations.weight[0].lora_A) * scale_term\n",
    "new_weights = model.linear1.parametrizations.weight.original + lora_term\n",
    "\n",
    "assert torch.equal(old_weights, new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "8SoWnRDgLBQM"
   },
   "outputs": [],
   "source": [
    "# If we disable lora, linear1.weight should be the original\n",
    "\n",
    "enable_disable_lora(enabled=False)\n",
    "assert torch.equal(model.linear1.weight, original_weights['linear1.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ki0l4z0hLdmy",
    "outputId": "dddae5c4-44d6-42a0-e7f2-9ccc78c6284d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:04<00:00, 225.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.853\n",
      "Wrong counts for digit 0: 10\n",
      "Wrong counts for digit 1: 44\n",
      "Wrong counts for digit 2: 51\n",
      "Wrong counts for digit 3: 97\n",
      "Wrong counts for digit 4: 10\n",
      "Wrong counts for digit 5: 64\n",
      "Wrong counts for digit 6: 83\n",
      "Wrong counts for digit 7: 131\n",
      "Wrong counts for digit 8: 146\n",
      "Wrong counts for digit 9: 833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test model with LoRA enabled (weight access uses parametrization)\n",
    "# If everything worked correctly, performance on digit 4 should increase\n",
    "\n",
    "enable_disable_lora(enabled=True)\n",
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j4b5LfsjL2fB",
    "outputId": "9accd556-79c6-450f-9710-f87d5726d612"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:03<00:00, 294.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.956\n",
      "Wrong counts for digit 0: 9\n",
      "Wrong counts for digit 1: 7\n",
      "Wrong counts for digit 2: 32\n",
      "Wrong counts for digit 3: 55\n",
      "Wrong counts for digit 4: 74\n",
      "Wrong counts for digit 5: 38\n",
      "Wrong counts for digit 6: 53\n",
      "Wrong counts for digit 7: 63\n",
      "Wrong counts for digit 8: 62\n",
      "Wrong counts for digit 9: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model with LoRA disabled\n",
    "# Should return to original performance\n",
    "\n",
    "enable_disable_lora(enabled=False)\n",
    "test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOziqQvtmW3Yjp7Xk8YZex2",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
